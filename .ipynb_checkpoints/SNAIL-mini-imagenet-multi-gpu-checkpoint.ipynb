{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-17T04:17:32.669231Z",
     "start_time": "2018-07-17T04:17:28.617266Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skinet/py_libs/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd, nd\n",
    "from mxnet.gluon import nn,utils\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "import mxnet.ndarray as F\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import os\n",
    "import numpy as np\n",
    "import collections\n",
    "from PIL import Image\n",
    "import csv\n",
    "import random\n",
    "from mxnet.gluon.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-17T04:17:34.779979Z",
     "start_time": "2018-07-17T04:17:34.772474Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mx.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-17T04:17:36.500721Z",
     "start_time": "2018-07-17T04:17:35.980356Z"
    }
   },
   "outputs": [],
   "source": [
    "class MiniImagenet(Dataset):\n",
    "    \"\"\"\n",
    "    put mini-imagenet files as :\n",
    "    root :\n",
    "        |- images/*.jpg includes all imgeas\n",
    "        |- train.csv\n",
    "        |- test.csv\n",
    "        |- val.csv\n",
    "        NOTICE: meta-learning is different from general supervised learning, especially the concept of batch and set.\n",
    "    batch: contains several sets\n",
    "    sets: conains n_way * k_shot for meta-train set, n_way * n_query for meta-test set.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, root, mode, batchsz, n_way, k_shot, k_query, resize, ctx=mx.cpu(), startidx=0):\n",
    "        \"\"\"\n",
    "\n",
    "        :param root: root path of mini-imagenet\n",
    "        :param mode: train, val or test\n",
    "        :param batchsz: batch size of sets, not batch of imgs\n",
    "        :param n_way:\n",
    "        :param k_shot:\n",
    "        :param k_query: num of qeruy imgs per class\n",
    "        :param resize: resize to\n",
    "        :param ctx: context\n",
    "        :param startidx: start to index label from startidx\n",
    "        \"\"\"\n",
    "\n",
    "        self.batchsz = batchsz  # batch of set, not batch of imgs\n",
    "        self.n_way = n_way  # n-way\n",
    "        self.k_shot = k_shot  # k-shot\n",
    "        self.k_query = k_query  # for evaluation\n",
    "        self.setsz = self.n_way * self.k_shot  # num of samples per set\n",
    "        self.querysz = self.n_way * self.k_query  # number of samples per set for evaluation\n",
    "        self.resize = resize  # resize to\n",
    "        self.ctx = ctx\n",
    "        self.startidx = startidx  # index label not from 0, but from startidx\n",
    "        print('shuffle DB :%s, b:%d, %d-way, %d-shot, %d-query, resize:%d' % (mode, batchsz, n_way, k_shot, k_query, resize))\n",
    "\n",
    "        if mode == 'train':\n",
    "            self.transform = transforms.Compose([lambda x: nd.array(np.array(Image.open(x).convert('RGB'))).astype(dtype=np.uint8),\n",
    "                                                 transforms.Resize((self.resize, self.resize)),\n",
    "                                                 # transforms.RandomHorizontalFlip(),\n",
    "                                                # transforms.RandomRotation(5),\n",
    "                                                 transforms.ToTensor(),\n",
    "                                                 transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "                                                 ])\n",
    "        else:\n",
    "            self.transform = transforms.Compose([lambda x: nd.array(np.array(Image.open(x).convert('RGB'))).astype(dtype=np.uint8),\n",
    "                                                 transforms.Resize((self.resize, self.resize)),\n",
    "                                                 transforms.ToTensor(),\n",
    "                                                 transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "                                                 ])\n",
    "\n",
    "        self.path = os.path.join(root, 'images')  # image path\n",
    "        csvdata = self.loadCSV(os.path.join(root, mode + '.csv'))  # csv path\n",
    "        self.data = []\n",
    "        self.img2label = {}\n",
    "        for i, (k, v) in enumerate(csvdata.items()):\n",
    "            self.data.append(v)  # [[img1, img2, ...], [img111, ...]]\n",
    "            self.img2label[k] = i + self.startidx  # {\"img_name[:9]\":label}\n",
    "        self.cls_num = len(self.data)\n",
    "\n",
    "        self.create_batch(self.batchsz)\n",
    "\n",
    "    def loadCSV(self, csvf):\n",
    "        \"\"\"\n",
    "        return a dict saving the information of csv\n",
    "        :param splitFile: csv file name\n",
    "        :return: {label:[file1, file2 ...]}\n",
    "        \"\"\"\n",
    "        dictLabels = {}\n",
    "        with open(csvf) as csvfile:\n",
    "            csvreader = csv.reader(csvfile, delimiter=',')\n",
    "            next(csvreader, None)  # skip (filename, label)\n",
    "            for i, row in enumerate(csvreader):\n",
    "                filename = row[0]\n",
    "                label = row[1]\n",
    "                # append filename to current label\n",
    "                if label in dictLabels.keys():\n",
    "                    dictLabels[label].append(filename)\n",
    "                else:\n",
    "                    dictLabels[label] = [filename]\n",
    "        return dictLabels\n",
    "\n",
    "    def create_batch(self, batchsz):\n",
    "        \"\"\"\n",
    "        create batch for meta-learning.\n",
    "        ×episode× here means batch, and it means how many sets we want to retain.\n",
    "        :param episodes: batch size\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.support_x_batch = []  # support set batch\n",
    "        self.query_x_batch = []  # query set batch\n",
    "        for b in range(batchsz):  # for each batch\n",
    "            # 1.select n_way classes randomly\n",
    "            selected_cls = np.random.choice(self.cls_num, self.n_way, False)  # no duplicate\n",
    "            np.random.shuffle(selected_cls)\n",
    "            support_x = []\n",
    "            query_x = []\n",
    "            for cls in selected_cls:\n",
    "                # 2. select k_shot + k_query for each class\n",
    "                selected_imgs_idx = np.random.choice(len(self.data[cls]), self.k_shot + self.k_query, False)\n",
    "                np.random.shuffle(selected_imgs_idx)\n",
    "                indexDtrain = np.array(selected_imgs_idx[:self.k_shot])  # idx for Dtrain\n",
    "                indexDtest = np.array(selected_imgs_idx[self.k_shot:])  # idx for Dtest\n",
    "                support_x.append(\n",
    "                    np.array(self.data[cls])[indexDtrain].tolist())  # get all images filename for current Dtrain\n",
    "                query_x.append(np.array(self.data[cls])[indexDtest].tolist())\n",
    "\n",
    "            # shuffle the correponding relation between support set and query set\n",
    "            random.shuffle(support_x)\n",
    "            random.shuffle(query_x)\n",
    "\n",
    "            self.support_x_batch.append(support_x)  # append set to current sets\n",
    "            self.query_x_batch.append(query_x)  # append sets to current sets\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        index means index of sets, 0<= index <= batchsz-1\n",
    "        :param index:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # [setsz, 3, resize, resize]\n",
    "        #support_x = torch.FloatTensor(self.setsz, 3, self.resize, self.resize)\n",
    "        support_x = nd.empty(shape=(self.setsz, 3, self.resize, self.resize))\n",
    "        # [setsz]\n",
    "        support_y = np.zeros((self.setsz), dtype=np.int)\n",
    "        # [querysz, 3, resize, resize]\n",
    "        #query_x = torch.FloatTensor(self.querysz, 3, self.resize, self.resize)\n",
    "        query_x = nd.empty(shape=(self.querysz, 3, self.resize, self.resize))\n",
    "        # [querysz]\n",
    "        query_y = np.zeros((self.querysz), dtype=np.int)\n",
    "\n",
    "        flatten_support_x = [os.path.join(self.path, item)\n",
    "                             for sublist in self.support_x_batch[index] for item in sublist]\n",
    "        support_y = np.array(\n",
    "            [self.img2label[item[:9]]  # filename:n0153282900000005.jpg, the first 9 characters treated as label\n",
    "            for sublist in self.support_x_batch[index] for item in sublist]).astype(np.int32)\n",
    "\n",
    "        flatten_query_x = [os.path.join(self.path, item)\n",
    "                           for sublist in self.query_x_batch[index] for item in sublist]\n",
    "        query_y = np.array([self.img2label[item[:9]]\n",
    "                            for sublist in self.query_x_batch[index] for item in sublist]).astype(np.int32)\n",
    "\n",
    "\n",
    "        # print('global:', support_y, query_y)\n",
    "        # support_y: [setsz]\n",
    "        # query_y: [querysz]\n",
    "        # unique: [n-way], sorted\n",
    "        unique = np.unique(support_y)\n",
    "        # relative means the label ranges from 0 to n-way\n",
    "        support_y_relative = np.zeros(self.setsz)\n",
    "        query_y_relative = np.zeros(self.querysz)\n",
    "        for idx, l in enumerate(unique):\n",
    "            support_y_relative[support_y==l] = idx\n",
    "            query_y_relative[query_y==l] = idx\n",
    "\n",
    "        # print('relative:', support_y_relative, query_y_relative)\n",
    "\n",
    "\n",
    "        for i, path in enumerate(flatten_support_x):\n",
    "            #print(path)\n",
    "            support_x[i] = self.transform(path)\n",
    "\n",
    "        for i, path in enumerate(flatten_query_x):\n",
    "            query_x[i] = self.transform(path)\n",
    "        # print(support_set_y)\n",
    "        # return support_x, torch.LongTensor(support_y), query_x, torch.LongTensor(query_y)\n",
    "\n",
    "        #return support_x, torch.LongTensor(support_y_relative), query_x, torch.LongTensor(query_y_relative)\n",
    "        return support_x, nd.array(support_y_relative,ctx=self.ctx), query_x, nd.array(query_y_relative,ctx=self.ctx)\n",
    "    def __len__(self):\n",
    "        # as we have built up to batchsz of sets, you can sample some small batch size of sets.\n",
    "        return self.batchsz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-17T04:17:37.254388Z",
     "start_time": "2018-07-17T04:17:37.243777Z"
    }
   },
   "outputs": [],
   "source": [
    "class CasualConv1d(nn.Block):\n",
    "    \n",
    "    def __init__(self,in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, bias=True,**kwargs):\n",
    "        super(CasualConv1d,self).__init__(**kwargs)\n",
    "        self.dilation = dilation\n",
    "        self.padding = dilation * (kernel_size - 1)\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.casual_conv = nn.Conv1D(in_channels=in_channels,channels=out_channels,kernel_size=kernel_size,padding = self.padding, dilation = dilation, groups=groups, use_bias=bias)\n",
    "            \n",
    "    def forward(self,x):\n",
    "        out = self.casual_conv(x)\n",
    "        return out[:,:,:-self.dilation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-17T04:17:37.652120Z",
     "start_time": "2018-07-17T04:17:37.640644Z"
    }
   },
   "outputs": [],
   "source": [
    "class DenseBlock(nn.Block):\n",
    "    def __init__(self, in_channels, filters, dilation=1, kernel_size=2, **kwargs):\n",
    "        super(DenseBlock,self).__init__(**kwargs)\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.casual_conv1 = CasualConv1d(in_channels, filters, kernel_size, dilation = dilation)\n",
    "            self.casual_conv2 = CasualConv1d(in_channels, filters, kernel_size, dilation = dilation)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        tanh = F.tanh(self.casual_conv1(x))\n",
    "        sigmoid = F.sigmoid(self.casual_conv1(x))\n",
    "        out =  F.concat(x,tanh*sigmoid, dim=1)\n",
    "        #print(\"Dense success\")\n",
    "        return out  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-17T04:17:37.907282Z",
     "start_time": "2018-07-17T04:17:37.895249Z"
    }
   },
   "outputs": [],
   "source": [
    "class TCBlock(nn.Block):\n",
    "    def __init__(self, in_channels,seq_len, filters, **kwargs):\n",
    "        super(TCBlock,self).__init__(**kwargs)\n",
    "        layer_count = int(math.ceil(math.log(seq_len)))\n",
    "        channel_count = in_channels\n",
    "        with self.name_scope():\n",
    "            self.blocks = nn.Sequential()\n",
    "            for i in range(layer_count):\n",
    "                self.blocks.add(DenseBlock(in_channels + i * filters, filters, dilation =2 ** (i+1)))\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = x.swapaxes(1,2)\n",
    "        out = self.blocks(x)\n",
    "        return out.swapaxes(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-17T04:17:38.206849Z",
     "start_time": "2018-07-17T04:17:38.138225Z"
    }
   },
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Block):\n",
    "    def __init__(self, k_size, v_size,ctx=mx.cpu(),show_shape=False, **kwargs):\n",
    "        super(AttentionBlock,self).__init__(**kwargs)\n",
    "        self.ctx = ctx\n",
    "        self.sqrt_k = math.sqrt(k_size)\n",
    "        self.show_shape = False\n",
    "        with self.name_scope():\n",
    "            self.key_layer = nn.Dense(k_size,flatten=False)\n",
    "            self.query_layer = nn.Dense(k_size,flatten=False)\n",
    "            self.value_layer = nn.Dense(v_size,flatten=False)\n",
    "            \n",
    "    \n",
    "    def forward(self, x):\n",
    "        with x.context:\n",
    "            #x = x.swapaxes(1,2)\n",
    "            keys = self.key_layer(x)       \n",
    "            queries = self.query_layer(x)\n",
    "            values = self.value_layer(x)\n",
    "            logits = nd.linalg_gemm2(queries,keys.swapaxes(2,1))\n",
    "            if self.show_shape:\n",
    "                print(\"keys shape:{}\".format(keys.shape))\n",
    "                print(\"queries shape:{}\".format(queries.shape))\n",
    "                print(\"logits shape:{}\".format(logits.shape))\n",
    "            #Generate masking part \n",
    "            mask = np.full(shape=(logits.shape[1],logits.shape[2]),fill_value=1).astype('float')\n",
    "            mask = np.triu(mask,1)\n",
    "            mask = np.expand_dims(mask,0)\n",
    "            mask = np.repeat(mask,logits.shape[0],0)\n",
    "            np.place(mask,mask==1,0.0)\n",
    "            np.place(mask,mask==0,1.0)\n",
    "            mask = nd.array(mask)\n",
    "            logits =  F.elemwise_mul(logits,mask)\n",
    "            probs = F.softmax(logits / self.sqrt_k, axis=2)\n",
    "            if self.show_shape:\n",
    "                print(\"probs shape:{}\".format(probs.shape))\n",
    "                print(\"values shape:{}\".format(values.shape))\n",
    "            read = nd.linalg_gemm2(probs,values)\n",
    "            concat_data = F.concat(x,read,dim=2)\n",
    "            return concat_data\n",
    "            #return queries,probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual_Block(nn.Block):\n",
    "    def __init__(self,filters, pool_padding=0,**kwargs):\n",
    "        super(Residual_Block,self).__init__( **kwargs)\n",
    "        with self.name_scope():\n",
    "            self.conv1 = nn.Conv2D(filters,kernel_size=3,strides=1,padding=1,use_bias=False)\n",
    "            self.bn1 =  nn.BatchNorm()\n",
    "            self.relu1 = nn.LeakyReLU(alpha=0.1)\n",
    "            self.conv2 = nn.Conv2D(filters,kernel_size=3,strides=1,padding=1,use_bias=False)\n",
    "            self.bn2 = nn.BatchNorm()\n",
    "            self.relu2 = nn.LeakyReLU(alpha=0.1)\n",
    "            self.conv3 = nn.Conv2D(filters,kernel_size=3,strides=1,padding=1,use_bias=False)\n",
    "            self.bn3 = nn.BatchNorm()\n",
    "            self.relu3 = nn.LeakyReLU(alpha=0.1)\n",
    "            self.conv4 = nn.Conv2D(filters,kernel_size=1,strides=1,padding=0,use_bias=False)\n",
    "            \n",
    "            self.max_pooling = nn.MaxPool2D(2, padding=pool_padding)\n",
    "            self.dropout = nn.Dropout(rate=0.9)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        residual = self.conv4(x)\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.relu3(out)\n",
    "        \n",
    "        out = out + residual\n",
    "        out = self.max_pooling(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-17T04:17:38.612380Z",
     "start_time": "2018-07-17T04:17:38.571049Z"
    }
   },
   "outputs": [],
   "source": [
    "class MiniImageNet_emb(nn.Block):\n",
    "    \n",
    "    def __init__(self,**kwargs):\n",
    "        super(MiniImageNet_emb,self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.block1 = Residual_Block(64)\n",
    "            self.block2 = Residual_Block(96)\n",
    "            self.block3 = Residual_Block(128, pool_padding=1)\n",
    "            self.block4 = Residual_Block(256, pool_padding=1)\n",
    "            self.conv1 = nn.Conv2D(2048,kernel_size=1,padding=1)\n",
    "            self.max_pooling = nn.MaxPool2D(6)\n",
    "            self.relu = nn.LeakyReLU(alpha=0)\n",
    "            self.dropout = nn.Dropout(rate=0.9)\n",
    "            self.conv2 = nn.Conv2D(384,kernel_size=1)\n",
    "            \n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = self.block1(x)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.block4(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.max_pooling(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv2(out)\n",
    "        return out.reshape(out.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-17T04:17:39.180136Z",
     "start_time": "2018-07-17T04:17:39.117187Z"
    }
   },
   "outputs": [],
   "source": [
    "class SNAIL(nn.Block):\n",
    "    def __init__(self,N,K,input_dims,**kwargs):\n",
    "        super(SNAIL,self).__init__(**kwargs)\n",
    "        self.N = N\n",
    "        self.K = K\n",
    "        self.num_filters = int(math.ceil(math.log(N * K + 1)))\n",
    "        self.ctx = ctx\n",
    "        self.num_channels = input_dims + N\n",
    "        with self.name_scope():\n",
    "            self.cnn_emb = MiniImageNet_emb()\n",
    "            self.attn1 = AttentionBlock(64, 32, ctx=self.ctx)\n",
    "            attn1_out_shape = self.num_channels + 32\n",
    "            self.tc1 = TCBlock(attn1_out_shape ,N*K+1 , 128)\n",
    "            tc1_out_shape = attn1_out_shape + self.num_filters * 128\n",
    "            self.attn2 = AttentionBlock(256, 128, ctx=self.ctx)\n",
    "            attn2_out_shape = tc1_out_shape + 128\n",
    "            self.tc2 = TCBlock(attn2_out_shape ,N*K+1 , 128)\n",
    "            tc2_out_shape = attn2_out_shape + self.num_filters * 128\n",
    "            self.attn3 = AttentionBlock(512, 256, ctx=self.ctx)\n",
    "            attn3_out_shape = tc2_out_shape + 128\n",
    "            self.fc = nn.Dense(N,flatten=False)\n",
    "                        \n",
    "    def forward(self, x, labels):\n",
    "        with x.context:\n",
    "            batch_size = int(labels.shape[0] / (N * K + 1))\n",
    "            last_idxs = [(i + 1) * (N * K + 1) - 1 for i in range(batch_size)]\n",
    "            labels[last_idxs] = nd.zeros(shape=(batch_size, labels.shape[1]))\n",
    "            x = self.cnn_emb(x)\n",
    "            x = F.concat(x,labels,dim=1)\n",
    "            x = x.reshape((batch_size,N*K+1,-1))\n",
    "            x = self.attn1(x)\n",
    "            x = self.tc1(x)\n",
    "            x = self.attn2(x)\n",
    "            x = self.tc2(x)\n",
    "            x = self.attn3(x)\n",
    "            x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-17T04:17:40.237490Z",
     "start_time": "2018-07-17T04:17:40.235356Z"
    }
   },
   "outputs": [],
   "source": [
    "os.chdir('/home/skinet/work/datasets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-17T04:17:40.648888Z",
     "start_time": "2018-07-17T04:17:40.641984Z"
    }
   },
   "outputs": [],
   "source": [
    "N = 10     #num_class\n",
    "K = 5  #num_samples\n",
    "k_query = 1\n",
    "iterations = 1000\n",
    "batch_size = 4\n",
    "GPU_INDEX = [4,5,6,7]\n",
    "ctx = [mx.gpu(i) for i in GPU_INDEX]\n",
    "\n",
    "iterations = int(iterations / len(ctx))\n",
    "batch_size = int(batch_size * len(ctx))\n",
    "epoches = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-17T04:17:41.501226Z",
     "start_time": "2018-07-17T04:17:41.453011Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffle DB :train, b:250, 10-way, 5-shot, 1-query, resize:84\n",
      "shuffle DB :test, b:250, 10-way, 5-shot, 1-query, resize:84\n"
     ]
    }
   ],
   "source": [
    "mini = MiniImagenet('mini-imagenet/', mode='train', n_way=N, k_shot=K, k_query=k_query, batchsz=iterations, resize=84)\n",
    "mini_test = MiniImagenet('mini-imagenet/', mode='test', n_way=N, k_shot=K, k_query=k_query, batchsz=iterations, resize=84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-17T04:17:42.024187Z",
     "start_time": "2018-07-17T04:17:42.021937Z"
    }
   },
   "outputs": [],
   "source": [
    "db = DataLoader(mini, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "db_test = DataLoader(mini_test, batch_size=batch_size, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snail_data_generation(batch, N):\n",
    "    iter_idx = 0\n",
    "    batch_size = batch[0].shape[0]\n",
    "    support_x = batch[0].asnumpy()\n",
    "    support_y = batch[1].asnumpy()\n",
    "    query_x = batch[2].asnumpy()\n",
    "    query_y =  batch[3].asnumpy()\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        query_idx = randint(0,N-1)\n",
    "        if iter_idx == 0 :\n",
    "            x = support_x[i]\n",
    "            x = np.append(x,np.expand_dims(query_x[i][query_idx],axis=0),axis=0)\n",
    "            y = support_y[i]\n",
    "            y = np.append(y,query_y[i][query_idx])\n",
    "            iter_idx +=1\n",
    "        else :\n",
    "            x = np.append(x,support_x[i],axis=0)\n",
    "            x = np.append(x,np.expand_dims(query_x[i][query_idx],axis=0),axis=0)\n",
    "            y = np.append(y,support_y[i])\n",
    "            y = np.append(y,query_y[i][query_idx])\n",
    "            iter_idx +=1\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-17T00:18:25.702017Z",
     "start_time": "2018-07-17T00:18:25.687513Z"
    }
   },
   "outputs": [],
   "source": [
    "def batch_for_few_shot(num_cls,num_samples,batch_size, x, y):\n",
    "    seq_size = num_cls * num_samples + 1\n",
    "    one_hots = []\n",
    "    last_targets = []\n",
    "    for i in range(batch_size):\n",
    "        one_hot, idxs = labels_to_one_hot(y[i * seq_size: (i + 1) * seq_size])\n",
    "        one_hots.append(one_hot)\n",
    "        last_targets.append(idxs[-1])\n",
    "    one_hots = [np.array(temp) for temp in one_hots]\n",
    "\n",
    "    y = np.concatenate(one_hots,0)\n",
    "    x = nd.array(x)\n",
    "    y = nd.array(y)\n",
    "    last_targets = nd.array(np.array(last_targets))\n",
    "    return x, y, last_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-17T00:18:27.362497Z",
     "start_time": "2018-07-17T00:18:27.356209Z"
    }
   },
   "outputs": [],
   "source": [
    "def labels_to_one_hot(labels):\n",
    "    labels = labels\n",
    "    unique = np.unique(labels)\n",
    "    map = {label:idx for idx, label in enumerate(unique)}\n",
    "    idxs = [map[labels[i]] for i in range(labels.size)]\n",
    "    one_hot = np.zeros((labels.size, unique.size))\n",
    "    one_hot[np.arange(labels.size), idxs] = 1\n",
    "    return one_hot, idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▋         | 1/16 [00:08<02:02,  8.15s/it]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▎        | 2/16 [00:15<01:45,  7.56s/it]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▉        | 3/16 [00:22<01:36,  7.40s/it]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▌       | 4/16 [00:29<01:29,  7.42s/it]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███▏      | 5/16 [00:37<01:22,  7.51s/it]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 6/16 [00:45<01:15,  7.54s/it]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▍     | 7/16 [00:53<01:08,  7.58s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 8/16 [01:00<01:00,  7.52s/it]\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████▋    | 9/16 [01:07<00:52,  7.53s/it]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▎   | 10/16 [01:15<00:45,  7.52s/it]\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████▉   | 11/16 [01:22<00:37,  7.50s/it]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▌  | 12/16 [01:29<00:29,  7.42s/it]\u001b[A\u001b[A\n",
      "\n",
      " 81%|████████▏ | 13/16 [01:37<00:22,  7.46s/it]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 14/16 [01:44<00:14,  7.49s/it]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▍| 15/16 [01:52<00:07,  7.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 16/16 [01:57<00:00,  7.33s/it]\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(tqdm(db_test)):\n",
    "    x,y = snail_data_generation(batch,N)\n",
    "    x, y, last_targets = batch_for_few_shot(N, K ,batch[0].shape[0], x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 50, 3, 84, 84)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16*50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-17T00:18:57.100866Z",
     "start_time": "2018-07-17T00:18:30.318158Z"
    }
   },
   "outputs": [],
   "source": [
    "model = SNAIL(N=N,K=K,input_dims=384)\n",
    "model.collect_params().initialize(ctx=ctx)\n",
    "loss_fn = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(model.collect_params(),optimizer='Adam',optimizer_params={'learning_rate':0.0001})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T04:49:55.864237Z",
     "start_time": "2018-07-12T02:18:54.587933Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-82d826b03682>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtr_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mte_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msnail_data_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_for_few_shot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.6/site-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    928\u001b[0m \"\"\", fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.6/site-packages/mxnet/gluon/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;31m# multi-worker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         return _MultiWorkerIter(self._num_workers, self._dataset,\n\u001b[0;32m--> 284\u001b[0;31m                                 self._batchify_fn, self._batch_sampler)\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.6/site-packages/mxnet/gluon/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_workers, dataset, batchify_fn, batch_sampler)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batchify_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatchify_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_sampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_sampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_key_queue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQueue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQueue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_buffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.6/site-packages/mxnet/gluon/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             super(Queue, self).__init__(*args, ctx=multiprocessing.get_context(),\n\u001b[0;32m---> 79\u001b[0;31m                                         **kwargs)\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConnectionWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_writer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConnectionWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_writer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, maxsize, ctx)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maxsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_writer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mduplex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplatform\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'win32'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/context.py\u001b[0m in \u001b[0;36mLock\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;34m'''Returns a non-recursive lock object'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msynchronize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mRLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/synchronize.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, ctx)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mSemLock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSEMAPHORE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/synchronize.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, kind, value, maxvalue, ctx)\u001b[0m\n\u001b[1;32m     58\u001b[0m                 sl = self._semlock = _multiprocessing.SemLock(\n\u001b[1;32m     59\u001b[0m                     \u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                     unlink_now)\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "train_acc = mx.metric.Accuracy()\n",
    "test_acc = mx.metric.Accuracy()\n",
    "global_va_acc = 0.0\n",
    "for epoch in range(epoches):\n",
    "    tr_acc = list()\n",
    "    te_acc = list()\n",
    "    for step, batch in enumerate(tqdm(db)):\n",
    "        x,y = snail_data_generation(batch,N)\n",
    "        x, y, last_targets = batch_for_few_shot(N, K ,batch[0].shape[0], x, y)       \n",
    "        with autograd.record():\n",
    "            x_split = gluon.utils.split_and_load(x,ctx)\n",
    "            y_split = gluon.utils.split_and_load(y,ctx)\n",
    "            last_targets_split = gluon.utils.split_and_load(last_targets,ctx)\n",
    "            last_model = [model(X,Y)[:,-1,:] for X, Y in zip(x_split,y_split)]\n",
    "            loss_val = [loss_fn(X,Y) for X, Y in zip(last_model,last_targets_split)]\n",
    "            #loss_val = [loss_fn(model(X,Y)[:,-1,:],L) for X, Y, L in zip(x_split,y_split,last_targets_split)]\n",
    "            for l in loss_val:\n",
    "                l.backward()\n",
    "            for pred,target in zip(last_model,last_targets_split):\n",
    "                train_acc.update(preds=nd.argmax(pred,1),labels=target)\n",
    "                tr_acc.append(train_acc.get()[1])\n",
    "        trainer.step(batch_size,ignore_stale_grad=True)\n",
    "        \n",
    "    for step, batch in enumerate(tqdm(db_test)):\n",
    "        x,y = snail_data_generation(batch,N)\n",
    "        x, y, last_targets = batch_for_few_shot(N, K ,batch[0].shape[0], x, y)\n",
    "        x = x.copyto(ctx[0])\n",
    "        y = y.copyto(ctx[0])\n",
    "        last_targets = last_targets.copyto(ctx[0])\n",
    "        model_output = model(x,y)\n",
    "        last_model = model_output[:,-1,:]\n",
    "        test_acc.update(preds=nd.argmax(last_model,1),labels=last_targets)\n",
    "        te_acc.append(test_acc.get()[1])\n",
    "    current_va_acc = np.mean(te_acc)\n",
    "    if global_va_acc < current_va_acc:\n",
    "        filename = '/home/skinet/work/research/SNAIL/imagenet_models/best_perf_epoch_'+str(epoch)+\"_tr_acc_\"+str(round(np.mean(tr_acc),2))+\"_te_acc_\"+str(round(np.mean(te_acc),2))\n",
    "        model.save_params(filename)\n",
    "        global_va_acc = current_va_acc\n",
    "    print(\"epoch {e}  train_acc:{ta} test_acc:{tea} \".format(e=epoch,ta=np.mean(tr_acc),tea=np.mean(te_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 50, 3, 84, 84)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = snail_data_generation(batch,N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(816,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(816, 3, 84, 84)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 50, 3, 84, 84)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-3e6a11c8bc43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msnail_data_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_for_few_shot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.6/site-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    928\u001b[0m \"\"\", fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.6/site-packages/mxnet/gluon/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_push_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"negative maxlength\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bad_message_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(tqdm(db_test)):\n",
    "    x,y = snail_data_generation(batch,N)\n",
    "    x, y, last_targets = batch_for_few_shot(N, K ,batch_size, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T06:09:43.715450Z",
     "start_time": "2018-07-10T06:09:43.711521Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1224, 10)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
